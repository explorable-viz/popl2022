\section{Introduction}

Programming languages researchers have developed a variety of tools and methods for helping programmers understand programs, ranging from provenance techniques and taint tracking to dataflow analysis and program slicing \Todo{citations}. These tools have a wide range of applications, including security, optimisation and program comprehension \Todo{citations}. With programs operating on \Todo{blah}, it is increasingly important to make it easy to understand and explore how programs transform richly structured inputs into richly structured outputs. For example visualisation code turns tables of data into charts and other graphics; array computations operate on large multidimensional structures; compilers turn abstract syntax trees into compiled code. These sorts of problem domain require analysis tools that let us focus on a particular part of the output to see what parts of the input contribute to it, as well as to see what part of the input affects what part of the output. \Todo{Some evidence that we need this.}

These problems are no longer just relevant to programmers. Every day we consume data, curated by journalists, scientists and policy makers into charts and other visual summaries, which informs our understanding of the world around us. But charts can be hard to interpret correctly, even with access to the relevant data source. For the sake of transparency and trust, it is vital that content consumers, as well as content creators, are able to easily see how aspects of the visual output relate both to data, and to other charts generated from the same data. Innocent (but devastating) mistakes such as transposing two columns of data have gone unnoticed for several years, even in widely cited papers~\cite{miller06}. Less innocent mistakes are deployed by politicians to mislead voters~\cite{fullfact19}. Understanding and trusting a visualisation, even for an expert, means knowing what its visual attributes actually \emph{represent}, which in turn involves the following two comprehension challenges:

\begin{enumerate}
  \item Identifying the mapping between data source and visual elements in the visualisation
  \item Understanding how different views of the same data are related
\end{enumerate}

From a program analysis perspective, each of these presents an interesting analysis problem. For (1), we want to be able to focus on a visual element and see what inputs contribute to it. This is a matter of selecting a part of the structured output and performing a backwards analysis that identifies parts of the input data and perhaps parts of the program that contribute to the selected output. (2) is more challenging; we want to be able to focus on a visual element in one chart or graphic and see what aspects of a different chart are computed using related inputs. This is a matter of again selecting a part of the structured output and performing a backwards analysis to identify the required inputs, but then also performing a forwards analysis to identify dependent parts of the other output.

[ORIGINAL VERSION]

To address this, much work in data visualization has been done to assist analysts understand the mapping between data source and visual elements in the visualization and understand how different views of the same data are related. This is typically done through data visualization lirbaries that do not utilize rich program analysis techniques.

[TODO: Talk more about the motivation for linking and the existing libraries for this.]
However, because data visualization libraries cannot analyze the host language, they resort to using internal DSLs where normal data transformation is encoded not as normal host language calculations, but as operations in the DSL. Wouldn't it be nice if we could write code to produce data visualization in a standard programming language and then be able to question how the structured output relates to structured inputs using a clever PL technique? For example, consider the following linked visualization example (Figure X):

\begin{figure}[H]
   {\includegraphics[scale=0.14]{fig/example/vis-linking.png}}
   \begin{lstlistings}
      let data = [ ... ]
      let barchart = data |> some transformation
      let linechart = data |> some other transformation
   \end{lstlistings}
   \caption{Linking cognate visualisations via common data dependencies}
   \label{fig:introduction:vis-linking}
\end{figure}


Here, the code is written in normal programming language. The technique presented in this paper makes it possible to auotmatically link the barchart to the linechart. This is done by propagating information backwards, from the barchart to the data and then forwards, from the data to the linechart. Moreover, our method has nice theoretical properties, meaning that (some good things).

\section{Introduction}

\noindent Even with the data and source code used to create the visualisation to hand, answering questions such as these is difficult, requiring time and expertise. The basic problem is that visualisations are opaque, disconnected from the data and computations used to create them. This situation would be significantly imporved if visualisations allowed a reader to explore the relationship to the underlying data through the visualisation itself, revealing the relevant connections on a need-to-know basis, as the reader interacts with it (see left-hand side of \figref{introduction:viz-linking} below):

Building this sort of ``data-linked'' visualisation by hand is possible, but is a significant undertaking, requiring intimate knowledge of the computational relationship between chart and data, and programming effort to expose that information to the reader. Manual approaches are also unlikely to be correct and lead to brittle solutions that need to be changed whenever the application logic changes. Given that a visualisation is a view computed from a data source, it seems plausible that we might adapt techniques from program analysis and data provenance to provide a runtime infrastructure that automatically supports linked selections. Then the data scientist or visualisation designer can concern themselves with cleaning, aggregating and presenting data, leaving the infrastructure to take care of linking visualisations to the underlying data sources.

\subsection{Linking visualisations to each other}

It is also common to use more than one view to present distinct but related aspects of data. (We say visualisations are \emph{cognate} when they are related in this way.) Geospatial applications like GeoDa~\cite{anselin06} and charting libraries like Plotly provide a view coordination feature called \emph{brushing and linking}~\cite{becker87}, where selections in one chart automatically select corresponding elements in the other, as an aid to comprehension. In \figref{introduction:vis-linking} below, selecting a bar on the left automatically selects all the related visual elements on the right. Although such coordination features are highly desirable, they are either baked into specific applications, or require programmer effort and therefore must be anticipated in advance by the chart designer. Moreover the linking is opaque, providing no direct way for the reader to see the data which underpins the relationship.

\subsection{Our contributions}

In this paper, we present new language-based data provenance techniques for linking visualisations to data, and to each other, in a fine-grained way. Our specific contributions are as follows:

\begin{itemize}[leftmargin=*]
   \item[--] a review of \emph{Galois slicing}, a program slicing framework with round-tripping properties appropriate to our problem, and an analysis of its shortcomings (\secref{background});
   \item[--] a new bidirectional dependency analysis inspired by Galois slicing, addressing these shortcomings, for a core calculus with lists and arrays, and a discussion of how the components of this analysis can be combined in various ways to link inputs to outputs and outputs to other outputs (\secref{core-language});
   \item[--] a richer surface language called \OurLanguage with familiar functional programming features, including piecewise definitions, pattern matching, list notation and list comprehensions, and an extension of our analysis to the desugaring (\secref{surface-language});
   \item[--] an implementation of Fluid in PureScript, and a discussion and evaluation of the strengths and weaknesses of our approach (\secref{implementation}).
\end{itemize}
