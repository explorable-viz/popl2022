\section{Introduction}

Techniques for analysing the dynamic dependency structure of programs have been fruitful, with applications ranging from information-flow security~\cite{sabelfeld03} and optimisation~\cite{kildall73} to debugging and program comprehension~\cite{weiser81,delucia96}. There are, however, few methods suitable for fine-grained analysis of richly structured outputs, such as data visualisations and multidimensional arrays. Dataflow analyses \cite{reps95} tend to focus on analysing variables rather than parts of structured values. Where-provenance~\cite{buneman01} and related data provenance techniques are fine-grained, but specific to relational query languages. Taint tracking \cite{newsome05} is also fine-grained, but works forwards from input to output. For many applications, it would be useful to be able to focus on a particular part of the output, and have an analysis isolate the input data pertinent only to that substructure.

This is a need that increasingly arises outside of traditional programming. Journalists and data scientists use programs to compute charts and other visual summaries from data, charts which must be interpreted by colleagues, policy makers and lay readers alike. Interpreting a chart correctly means understanding what the components of the visualisation actually \emph{represent}, i.e.~the mapping between data and visual elements. But this is a hard task, requiring time and expertise, even with access to the data and source code used to create the visualisation. In practice it is easy for innocent (but devastating) mistakes such as transposing two columns of data to go unnoticed for several years~\cite{miller06}, or for unscrupulous politicians to misrepresent information to voters~\cite{fullfact19}.

\subsection{Fine-grained linking of outputs to inputs}

The situation would be much improved if a computed artefact such as a chart or summary allowed an end-user to explore the relationship to the underlying data through the artefact itself, revealing the relevant data on a need-to-know basis, as the user interacts with it. For example in \figref{introduction:data-linking} below, selecting a particular bars in the bar chart presents the user with a tabular summary of the data relevant to that bar:

\begin{figure}[H]
   {\includegraphics[scale=0.05]{fig/example/data-linking.png}}
   \caption{Linking structured outputs to structured inputs}
   \label{fig:introduction:data-linking}
\end{figure}

Indeed, visualisation designers do sometimes create ``data-linked'' artefacts like these by hand, such as Nadieh Bremer's award-winning visualisation of population density growth in Asian cities~\cite{bremer15}. But crafting such things by hand is a significant undertaking, requiring intimate knowledge of the computational relationship between chart and data, and programming effort to expose that information to the reader. Manual approaches are also error-prone and need to be changed whenever the application logic changes. A library-based approach would improve on this, but would only provide data linking for solutions that expressible using the combinators provided by the library.

An alternative is to frame this as a program analysis problem: we want to be able to focus on a particular visual element, such as a bar in the bar chart above, and determine the inputs that contribute to it. This is a matter of selecting a part of the structured output and performing some kind of backwards analysis that identifies the relevant data. The advantage of this approach is that the data scientist or visualisation designer gets data linking ``for free'', and can code their visualisations just as pure functions of the data using the full power of the language.

\subsection{Fine-grained linking between outputs}

It is also common to use more than one view to present distinct but related aspects of data. In this situation a reader needs to be able to focus on a visual element in one chart or graphic and isolate those aspects of a different chart which were computed using related inputs. Indeed, much work in data visualisation has been done towards this goal, typically done through visualisation libraries rather than program analysis techniques. One such feature, called \emph{brushing and linking}~\cite{becker87}, is common in geospatial applications like GeoDa~\cite{anselin06} and charting libraries like Plotly, where selections in one chart automatically select corresponding elements in the other. In \figref{introduction:vis-linking} below, selecting a bar on the left automatically selects all the related visual elements on the right. Although such ``view coordination'' features are highly desirable, they are either baked into specific applications, or require programmer effort and therefore must be anticipated in advance by the chart designer. Moreover the linking is opaque, providing no direct way for the reader to see the data which underpins the relationship.

\begin{figure}[H]
   {\includegraphics[scale=0.14]{fig/example/vis-linking.png}}
   \small
   \begin{lstlisting}
      let data = [ ... ]
      let barchart = data |> some transformation
      let linechart = data |> some other transformation
   \end{lstlisting}
   \caption{Linking visualisations via common data dependencies}
   \label{fig:introduction:vis-linking}
\end{figure}

We can again frame this as a program analysis problem. It is a matter of selecting a part of the structured output and performing a backwards analysis to identify the required inputs, as before, but then performing a further forwards analysis to identify dependent parts of the other output. The is in \figref{introduction:vis-linking} above. This would not only provided a language-based foundation for automatic brushing and linking, but would also provide a ``data-transparent'' version of brushing and linking, able to provide a concise view of the data required to explain the link.

\subsection{Contributions}

To solve these problems, we need a bidirectional dataflow analysis tailored to structured data in general-purpose languages. Recent program slicing techniques \cite{perera12a,perera13a,ricciotti17} allow the user to focus on the output by ``erasing'' parts deemed to be irrelevant; the erased parts, called \emph{holes}, are propagated backwards by a backwards analysis which identifies parts of the program and input which are no longer needed. Although these approaches enjoy useful round-tripping properties characterised by Galois connections, they only allow focusing on \emph{prefixes} of a structured output, rather than arbitrary substructures.

In this paper, we present new language-based data provenance techniques for linking structured outputs, such as visualisations, to structured inputs, and to each other, in a fine-grained way. Our specific contributions are as follows:

\begin{itemize}[leftmargin=*]
   \item[--] a review of \emph{Galois slicing}, a program slicing framework with round-tripping properties appropriate to our problem, and an analysis of its shortcomings (\secref{background});
   \item[--] a new bidirectional dependency analysis inspired by Galois slicing, addressing these shortcomings, for a core calculus with lists and arrays, and a discussion of how the components of this analysis can be combined in various ways to link inputs to outputs and outputs to other outputs (\secref{core-language});
   \item[--] a richer surface language called \OurLanguage with familiar functional programming features, including piecewise definitions, pattern matching, list notation and list comprehensions, and an extension of our analysis to the desugaring (\secref{surface-language});
   \item[--] an implementation of Fluid in PureScript, and a discussion and evaluation of the strengths and weaknesses of our approach (\secref{implementation}).
\end{itemize}
