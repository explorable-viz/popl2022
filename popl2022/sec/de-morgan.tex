\section{Galois connections for brushing and linking}
\label{sec:de-morgan}

The second problem we set out in \secref{introduction} was the idea of linking selections between outputs computed from the same data. (We call two such outputs \emph{cognate}.) This so-called ``brushing and linking'' problem~\cite{becker87} has been extensive studied as an interaction paradigm in data visualisation, with the emphasis on visualisation techniques and its value as a comprehension tool, rather than on infrastructure for automation. Intuitively, brushing and linking has a bidirectional flavour: one must consider how dependencies flow backward from a selection in one output to a selection $v$ in the common data, and then forward from the selected data $v$ to a corresponding selection in the other output. A natural question then is whether the bidirectional dependency analysis established in \secref{data-dependencies} can supply the information required to support an automated solution to brushing and linking.

An immediate problem is that the flavour of the forward dependency required here differs from that provided by the forward analysis $\evalFwdF{T}$ defined in \secref{data-dependencies:analyses:fwd}. That was able to answer the question: what can we compute given only the data selected in $v$? But to identify the related data in another output, we must determine not what the input selection $v$ is \emph{sufficient} for, but what it is \emph{necessary} for: those parts of the other output that depend on $v$. In fact the question is a kind of dual: what would we \emph{not} be able to compute if the data selected in $v$ were \emph{unavailable}?

\subsection{De Morgan duality}
\label{sec:de-morgan:de-morgan-duality}

Why $\evalFwdF{T}$ is unsuitable as a forward dependency relation for linking cognate outputs can also be understood in terms of compositionality. Suppose $\Ann{V}_1$ and $\Ann{V}_2$ are the lattices of selections for two views computed from a shared input source, and $\Ann{D}$ is the lattice of selections for the shared input. Using the procedure given in \secref{data-dependencies}, we can obtain two Galois connections $f: \Ann{V}_1 \to \Ann{D}$ and $g: \Ann{V}_2 \to \Ann{D}$ as shown in \figref{example:de-morgan:non-composable} below:

\input{fig/example/de-morgan.tex}

\noindent Unfortunately, $f$ and $g$ are not composable, as their types makes clear. While the upper adjoint $\upperAdj{g}: \Ann{D} \to \Ann{V}_2$ has the right type to compose with the lower adjoint $\lowerAdj{f}: \Ann{V}_1 \to \Ann{D}$, the result is not a Galois connection: $\upperAdj{g}$ preserves meets, whereas $\lowerAdj{f}$ preserves joins.

However, it turns out that if selections are closed under negation (complement), we can derive an operation that inverts $g$, yielding a Galois connection $\dual{g}$ that it is composable with $f$. Moreover, $\dual{g}$ computes a dependency relation corresponding to necessity rather than sufficiency. By composing $f$ with $\dual{g}$ we obtain a Galois connection linking $\Ann{V}_1$ to $\Ann{V}_2$ via $\Ann{D}$, as shown in \figref{example:de-morgan:composable}.

First we shift settings from the lattices used in \secref{data-dependencies} to Boolean lattices (or Boolean algebras) $\Ann{A}= \BoolLattice{\Ann{A}}{\top}{\bot}{\meet}{\join}{\neg}$, which are lattices equipped with an involution $\neg: \Ann{A} \to \Ann{A}$ called \emph{complement}. Boolean algebras satisfy complementation laws $x \meet \neg x = \bot$ and $x \join \neg x = \top$ and De Morgan laws $\neg x \meet \neg y = \neg(x \join y)$ and $\neg x \join \neg y = \neg(x \meet y)$. If $\Ann{A}$ is a Boolean algebra, then $\Sel{\raw{v}}{A}$ is also a Boolean algebra, with the Boolean operations, and in particular $\neg_{\raw{v}}: \Sel{\raw{v}}{A} \to \Sel{\raw{v}}{A}$, defined pointwise. We add an additional distinguished element $\blackhole$ to the selectable values to serve as the negation of $\hole$. The two-point lattice $\Bool$ we used to illustrate \secref{data-dependencies} is also a Boolean algebra $\BoolLattice{\Bool}{\TT}{\FF}{\wedge}{\vee}{\neg}$ with $\neg$ corresponding to logical negation.

\input{fig/example/convolve}

It is an easy consequence of the complementation and De Morgan laws that any meet-preserving operation $g: \Ann{A} \to \Ann{B}$ on Boolean algebras has a join-preserving De Morgan dual $\dual{g}: \Ann{A} \to \Ann{B}$ given by $\neg_{\Ann{B}} \after g\after \neg_{\Ann{A}}$, and any join-preserving operation $h$ has a meet-preserving De Morgan dual $\dual{h}$ defined similarly. Moreover if $h$ is the lower adjoint of $g$, then $\dual{g}$ is the lower adjoint of $\dual{h}$. Thus Galois connections on Boolean algebras also admit a (contravariant) notion of De Morgan duality, defined component-wise.

\begin{definition}[De Morgan dual of a Galois connection]
   Suppose $\Ann{A}$ and $\Ann{B}$ are Boolean algebras and $f: \Ann{A} \to \Ann{B}$ is a Galois connection $(\lowerAdj{f},\upperAdj{f})$. Define the \emph{De Morgan dual} $\dual{f}$ of $f$ to be the Galois connection $(\dual{\upperAdj{f}}, \dual{\lowerAdj{f}}): \Ann{B} \to \Ann{A}$.
\end{definition}

\noindent Dualising a Galois connection flips the direction of the arrow by swapping the roles of the upper and lower adjoints. So while $f: \Ann{A} \to \Ann{B}$ and $g: \Ann{C} \to \Ann{B}$ are not composable, $f$ and $\dual{g}: \Ann{B} \to \Ann{C}$ are, and the composition is achieved by transforming $\upperAdj{g}$ from something which determines what we can compute with $v$ into something which determines what we cannot compute without $v$. This offers a principled basis for a fully automated brushing and linking feature between cognate computations $T$ and $U$. When the user selects part of the output of $T$, we can use $\evalBwdF{T}$ to compute the needed data $v$, and then use $\dual{\evalFwdF{U}}$ to compute the parts of the output of $U$ that depend on $v$. This is the approach implemented in \OurLanguage, which we used to generate \figref{introduction:vis-linking}.

\subsection{Example: matrix convolution}

We now illustrate the $\dual{\evalGC{T}}$ Galois connection, contrasting it with $\evalGC{T}$, using an example which computes the convolution of a $5 \times 5$ matrix with a $3 \times 3$ kernel. Convolution has an intuitive dependency structure and the values involved have an easy visual presentation, making it useful for conveying the flavour of the four distinct (but connected) dependency relations that arise in the framework. The source code for the example is given in \figref{example:convolve}, and shows the \kw{convolve} function, plus \kw{zero}, \kw{wrap} and \kw{extend} which provide different methods for handling the boundaries of the input matrix. The angle-bracket notation is used to construct matrices, which were omitted from \secref{core-language}. (The formal treatment is similar to records.)

\input{fig/example/convolve-viz}

\OurLanguage{} was used to generate the diagrams in \figref{example:convolve-viz}, which show the four dependency relations and two of their four possible round-trips. \figref{example:convolve-viz:galois-dependency} shows the $\evalGC{T}$ Galois connection defined in \secref{data-dependencies:analysis:galois-connections}. In the upper figure (counting rows downwards from $1$), the user selects (in green) the output cell at position $(2,2)$. This induces a demand (via the lower adjoint $\evalBwdF{T}$) on the input matrix \kw{image} and the kernel \kw{filter}, revealing (in yellow) that the entire kernel was needed to compute the value $1$, but that the elements at $(1,3)$ and $(3,1)$ in the input matrix were not, because of zeros present in the kernel. If we then ``round-trip'' that input selection, computing the corresponding availability on the output using the upper adjoint $\evalFwdF{T}$, the green selection grows: it turns out that the data needed to make $(2,2)$ available are sufficient to make $(1,1)$ available as well.

\figref{example:convolve-viz:de-morgan-dual} shows the De Morgan dual $\dual{\evalGC{T}}$. In the upper part of the figure, the user selects (green) kernel cell $(1, 2)$ to see the output cells that depend on it. This is computed using the De Morgan dual of $\evalFwdF{T}$. First we negate the input selection, marking $(1, 2)$ as unavailable, and all other inputs as available. Then we forward-analyse with $\evalFwdF{T}$ to determine that if all inputs are available except kernel cell $(1, 2)$, we can compute only the top row of the output. (If it seems odd that we still still compute even the top row, notice that the example uses the method \kw{zero} for dealing with boundaries; \kw{wrap} or \kw{extend} would have given a different behaviour.) Then we negate that top row selection to produce the (yellow) output selection shown in the figure. These are exactly the output cells which depend on kernel cell ($1, 2)$ in the sense that they cannot be computed if that input is unavailable.

We can then round-trip this output selection using the De Morgan dual of $\evalBwdF{T}$. We first negate the yellow output selection (selecting the top row of the output again), and then use $\evalBwdF{T}$ to determine the needed inputs, which turn out to be the top two rows of \kw{image}, and the top row of \kw{filter}. Negating again produces the green output selection shown in the lower figure. Thus the backwards De Morgan dual computes the inputs that would \emph{not} be needed if the selected outputs were not needed: in other words inputs that are \emph{only} needed for the selected output. Here the round-trip reveals that if kernel cell $(1, 2)$ is unavailable, then the entire top row of the kernel might as well have been unavailable too, and similarly for the bottom 3 rows of the input.

\subsection{Relationship to Galois slicing}
\label{sec:de-morgan:galois-slicing}

\input{fig/example/diff-slicing}

The De Morgan dual puts us in a better position to consider the relationship between the present system and earlier work on \emph{Galois slicing}, a program slicing technique that has been explored for pure functional programs~\cite{perera12a}, functional programs with effects~\cite{ricciotti17}, and \piCalculus~\cite{perera16d}. We consider other related work in \secref{conclusion}.

Galois slicing operates on lattices of \emph{slices}, which are programs (or values) where parts deemed irrelevant are replaced by a hole $\hole$. (If we think of the notion of selection defined in \secref{data-dependencies:selections} as picking out a subset of the paths in a term, then slices resembles selections which are prefix-closed, meaning that if a given path in a term is selected, then so are all of its prefixes.) Then, for a fixed computation, a meet-preserving \emph{forward-slicing} function is defined taking input slices to output slices, discarding parts which cannot be computed because the needed input is not present, plus a join-preserving \emph{backward-slicing} function taking output slices to input slices, retaining the parts needed for the output slice. For example \figref{example:diff-slicing:original} shows a computation with output \lstinline{(0.4, 0.6)}, and \figref{example:diff-slicing:subtree} gives the backward slice for output slice \kw{(0.4, $\hole$)}. Forward and backward slicing, for a given computation, form a Galois connection, giving the analyses the nice round-tripping properties we motivated in \secref{data-dependencies:analysis:galois-connections}.

Unfortunately, the notion of slice does not lend itself to computing dependencies where the needed input or output is a proper part of a value, such as a component of a tuple. \emph{Differential} slicing \cite{perera12a} improves on this somewhat by using Galois slicing to compute a pair of input slices $(e,e'$) for a pair of output slices $(v,v')$ where $v \leq v'$. By monotonicity, $e \leq e'$. This can be used to compute a (differential) slice for an arbitrary subtree, by setting $v$ to be the ``spine'' of the original output up to the location of the subtree, and $v'$ to be $v$ with the subtree of interest plugged back in. So here we can focus on the value \kw{0.4} in the output by computing the backward slice for $\kw{($\hole$, $\hole$)}$ (\figref{example:diff-slicing:spine}) and then comparing it with the backward slice for \kw{(0.4, $\hole$)}, generating the differential slice shown in \figref{example:diff-slicing:differential}, where the parts that are different are highlighted in yellow. But although it supports a more precise notion of selection, the differential slice highlights a program part if it is needed \emph{only} by the selected output. Here, because \lstinline{2} and \lstinline{3} are both needed to compute the spine (in order to decide which conditional branch to execute), the differential slice does not include them. In fact differential slicing is similar to the dual of $\evalBwdF{T}$, and thus is not as it stands able to compute the dependency information we need for data linking.
