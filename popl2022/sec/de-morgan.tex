\section{Galois connections for brushing and linking}
\label{sec:de-morgan}

The second problem we set out in \secref{introduction} was the idea of linking selections between outputs computed from the same data. (We call two such outputs \emph{cognate}.) This so-called ``brushing and linking'' problem~\cite{becker87} has been extensive studied as an interaction paradigm in data visualisation, with the emphasis on visualisation techniques and its value as a comprehension tool, rather than on infrastructure for automation. Intuitively, brushing and linking has a bidirectional flavour: one must consider how dependencies flow backward from a selection in one output to a selection $v$ in the common data, and then forward from the selected data $v$ to a corresponding selection in the other output. A natural question then is whether the bidirectional dependency analysis established in \secref{data-dependencies} can supply the information required to support an automated solution to brushing and linking.

An immediate problem is that the flavour of the forward dependency required here differs from that provided by the forward analysis $\evalFwdF{T}$ defined in \secref{data-dependencies:analyses:fwd}. That was able to answer the question: what can we compute given only the data selected in $v$? But to identify the related data in another output, we must determine not what the input selection $v$ is \emph{sufficient} for, but what it is \emph{necessary} for: those parts of the other output that depend on $v$. In fact the question is a kind of dual: what would we \emph{not} be able to compute if the data selected in $v$ were \emph{unavailable}?

\subsection{De Morgan duality}
\label{sec:de-morgan:de-morgan-duality}

Why $\evalFwdF{T}$ is unsuitable as a forward dependency relation for linking cognate outputs can also be understood in terms of compositionality. Suppose $\Ann{V}_1$ and $\Ann{V}_2$ are the lattices of selections for two views computed from a shared input source, and $\Ann{D}$ is the lattice of selections for the shared input. Using the procedure given in \secref{data-dependencies}, we can obtain two Galois connections $f: \Ann{V}_1 \to \Ann{D}$ and $g: \Ann{V}_2 \to \Ann{D}$ as shown in \figref{example:de-morgan:non-composable} below:

\input{fig/example/de-morgan.tex}

\noindent Unfortunately, $f$ and $g$ are not composable, as their types makes clear. While the upper adjoint $\upperAdj{g}: \Ann{D} \to \Ann{V}_2$ has the right type to compose with the lower adjoint $\lowerAdj{f}: \Ann{V}_1 \to \Ann{D}$, the result is not a Galois connection: $\upperAdj{g}$ preserves meets, whereas $\lowerAdj{f}$ preserves joins.

However, it turns out that if selections are closed under negation (complement), we can derive an operation that inverts $g$, yielding a Galois connection $\dual{g}$ that it is composable with $f$. Moreover, $\dual{g}$ computes a dependency relation corresponding to necessity rather than sufficiency. By composing $f$ with $\dual{g}$ we obtain a Galois connection linking $\Ann{V}_1$ to $\Ann{V}_2$ via $\Ann{D}$, as shown in \figref{example:de-morgan:composable}.

First we shift settings from the lattices used in \secref{data-dependencies} to Boolean lattices (or Boolean algebras) $\Ann{A}= \BoolLattice{\Ann{A}}{\top}{\bot}{\meet}{\join}{\neg}$, which are lattices equipped with an involution $\neg: \Ann{A} \to \Ann{A}$ called \emph{complement}. Boolean algebras satisfy complementation laws $x \meet \neg x = \bot$ and $x \join \neg x = \top$ and De Morgan laws $\neg x \meet \neg y = \neg(x \join y)$ and $\neg x \join \neg y = \neg(x \meet y)$. If $\Ann{A}$ is a Boolean algebra, then $\Sel{\raw{v}}{A}$ is also a Boolean algebra, with the Boolean operations, and in particular $\neg_{\raw{v}}: \Sel{\raw{v}}{A} \to \Sel{\raw{v}}{A}$, defined pointwise. We add an additional distinguished element $\blackhole$ to the selectable values to serve as the negation of $\hole$. The two-point lattice $\Bool$ we used to illustrate \secref{data-dependencies} is also a Boolean algebra $\BoolLattice{\Bool}{\TT}{\FF}{\wedge}{\vee}{\neg}$ with $\neg$ corresponding to logical negation.

\input{fig/example/convolve}

It is an easy consequence of the complementation and De Morgan laws that any meet-preserving operation $g: \Ann{A} \to \Ann{B}$ on Boolean algebras has a join-preserving De Morgan dual $\dual{g}: \Ann{A} \to \Ann{B}$ given by $\neg_{\Ann{B}} \after g\after \neg_{\Ann{A}}$, and any join-preserving operation $h$ has a meet-preserving De Morgan dual $\dual{h}$ defined similarly. Moreover if $h$ is the lower adjoint of $g$, then $\dual{g}$ is the lower adjoint of $\dual{h}$. Thus Galois connections on Boolean algebras also admit a (contravariant) notion of De Morgan duality, defined component-wise.

\begin{definition}[De Morgan dual of a Galois connection]
   Suppose $\Ann{A}$ and $\Ann{B}$ are Boolean algebras and $f: \Ann{A} \to \Ann{B}$ is a Galois connection $(\lowerAdj{f},\upperAdj{f})$. Define the \emph{De Morgan dual} $\dual{f}$ of $f$ to be the Galois connection $(\dual{\upperAdj{f}}, \dual{\lowerAdj{f}}): \Ann{B} \to \Ann{A}$.
\end{definition}

\noindent Dualising a Galois connection flips the direction of the arrow by swapping the roles of the upper and lower adjoints. So while $f: \Ann{A} \to \Ann{B}$ and $g: \Ann{C} \to \Ann{B}$ are not composable, $f$ and $\dual{g}: \Ann{B} \to \Ann{C}$ are, and the composition is achieved by transforming $\upperAdj{g}$ from something which determines what we can compute with $v$ into something which determines what we cannot compute without $v$. This offers a principled basis for a fully automated brushing and linking feature between cognate computations $T$ and $U$. When the user selects part of the output of $T$, we can use $\evalBwdF{T}$ to compute the needed data $v$, and then use $\dual{\evalFwdF{U}}$ to compute the parts of the output of $U$ that depend on $v$. This is the approach implemented in \OurLanguage, which we used to generate \figref{introduction:vis-linking}.

\subsection{Example: matrix convolution}

We now illustrate the $\dual{\evalGC{T}}$ Galois connection, contrasting it with $\evalGC{T}$, using an example which computes the convolution of a $5 \times 5$ matrix with a $3 \times 3$ kernel. Convolution has an intuitive dependency structure and the values involved have an easy visual presentation, making it useful for conveying the flavour of the four distinct (but connected) dependency relations that arise in the framework. The source code for the example is given in \figref{example:convolve}, and shows the \kw{convolve} function, plus \kw{zero}, \kw{wrap} and \kw{extend} which provide different methods for handling the boundaries of the input matrix. The angle-bracket notation is used to construct matrices, which were omitted from \secref{core-language}. (The formal treatment is similar to records.)

\input{fig/example/convolve-viz}

\OurLanguage{} was used to generate the diagrams in \figref{example:convolve-viz}, which show the four dependency relations and two of their four possible round-trips. \figref{example:convolve-viz:galois-dependency} shows the $\evalGC{T}$ Galois connection defined in \secref{data-dependencies:analysis:galois-connections}. In the upper figure (counting rows downwards from $1$), the user selects (in green) the output cell at position $(2,2)$. This induces a demand (via the lower adjoint $\evalBwdF{T}$) on the input matrix \kw{image} and the kernel \kw{filter}, revealing (in yellow) that the entire kernel was needed to compute the value $1$, but that the elements at $(1,3)$ and $(3,1)$ in the input matrix were not, because of zeros present in the kernel. If we then ``round-trip'' that input selection, computing the corresponding availability on the output using the upper adjoint $\evalFwdF{T}$, the green selection grows: it turns out that the data needed to make $(2,2)$ available are sufficient to make $(1,1)$ available as well. These larger selections that arise from round-tripping are \emph{closed elements} (fixed points).

\figref{example:convolve-viz:de-morgan-dual} shows the De Morgan dual $\dual{\evalGC{T}}$. Here the user selects (green) kernel cell $(1, 2)$ to see the output cells that depend on it. This is computed in three steps. First we negate the input selection, marking the contents of $(1, 2)$ as unavailable, and all other inputs as available. Then we forward-analyse with $\evalFwdF{T}$ to determine that if all inputs are available except kernel cell $(1, 2)$, we can compute only the top row of the output. (If it seems odd that we still still compute even the top row, notice that the example uses the method \kw{zero} for dealing with boundaries; \kw{wrap} or \kw{extend} would have given a different behaviour.) The third step is to negating that top row selection to produce the (yellow) output selection shown in the figure. These are exactly the output cells which depend on kernel cell ($1, 2)$ in the sense that they cannot be computed if that input is unavailable.

If we then use this output selection to compute an input selection using the upper adjoint $\upperAdj{\evalGC{T}}$, the result is not a \emph{demand} in the sense computed by the lower adjoint $\evalBwdF{T}$. Instead of determining the inputs needed for the output selection, it determines the inputs that would \emph{not} be needed if those outputs were not needed: in other words, those inputs that are \emph{only} needed for the output selection. Here the input selection grows, revealing that if kernel cell $(1, 2)$ is unavailable, then the entire top row of the kernel might as well have been unavailable too, and similarly for the bottom 3 rows of the input. Again these larger selections that result from round-tripping are fixed points.

\subsection{Relationship to Galois slicing}
\label{sec:de-morgan:galois-slicing}

With the De Morgan dual, our approach addresses the two main problems we set out to solve in \secref{introduction}. In the next section we show how we can track data dependencies back to related surface language constructors, but first we briefly examine the relationship between the present system and earlier work on ``Galois slicing'', a program slicing technique that has been explored for pure functional programs~\cite{perera12a}, functional programs with effects~\cite{ricciotti17}, and \piCalculus~\cite{perera16d}. We consider other related work in \secref{conclusion}.

\input{fig/example/diff-slicing}

Galois slicing has the flavour of what we needed to address these problems, but is unable to compute the kind of dependency relation we gave in \secref{data-dependencies:analyses:bwd}. Galois slicing works with a notion of \emph{program slice} defined as a program where some subexpressions have been replaced by hole $\hole$. (If we think of a selection, in our sense, as picking out a set of paths in a term, then slices resembles selections which are prefix-closed, meaning that if a given path in a term is selected, then so are all of its prefixes.) Then a meet-preserving \emph{forward-slicing} function (for a fixed computation) is defined which takes a slice of the original program to a slice of its output, a value where some subvalues have been replaced by $\hole$, and dual to this, a join-preserving \emph{backward-slicing} function taking value slices program slice. Forward and backward slicing, for a given computation, form a Galois connection, establishing the important correctness properties that validate the approach.

One problem with this approach is that it does not readily extend to a notion of selection where the part of the output of interest is not a prefix of the output, but rather a prefix of some subtree. \emph{Differential} slicing \cite{perera12a} uses Galois slicing to compute a pair of $(e,e'$) for a pair of output slices $(v,v')$ where $v \leq v'$. By monotonicity, $e \leq e'$. This can be used to compute a slice for a subtree, by setting $v'$ to be the ``spine'' of the original output up to the location of the subtree with a $\hole$ instead of the subtree, and $v'$ is $v$ with the subtree plugged into that position. For example, consider the program shown in \figref{example:diff-slicing}, which has the value \lstinline{(0.4, 0.6)}. Because differential slicing includes a program part if is needed \emph{only} by the selected output, in general it underapproximates the parts actually needed for the selected output. In this example, \lstinline{2} and \lstinline{3} are both needed to compute the spine containing the subtree of interest, and so the differential slice does not include them. Differential slicing based on tree prefixes is thus not a suitable technique for computing data dependencies.
