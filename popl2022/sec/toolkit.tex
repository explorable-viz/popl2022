\section{Galois connections for brushing and linking}
\label{sec:toolkit}

The second problem we set out in \secref{introduction} was the idea of linking selections between outputs computed from the same data. (We call two such outputs \emph{cognate}.) This so-called ``brushing and linking'' problem~\cite{becker87} has been extensive studied as an interaction paradigm in data visualisation, with the emphasis on visualisation techniques and its value as a comprehension tool, rather than on infrastructure for automation. Intuitively, brushing and linking has a bidirectional flavour: one must consider how dependencies flow backward from a selection in one output to a selection $v$ in the common data, and then forward from the selected data $v$ to a corresponding selection in the other output. A natural question then is whether the bidirectional dependency analysis established in \secref{data-dependencies} can supply the information required to support an automated solution to brushing and linking.

An immediate problem is that the flavour of the forward dependency required here differs from that provided by the forward analysis $\evalFwdF{T}$ defined in \secref{data-dependencies}. That was able to answer the question: what can we compute given only the data selected in $v$? But to identify the related data in another output, we must determine not what the input selection $v$ is \emph{sufficient} for, but what it is \emph{necessary} for: those parts of the other output that depend on $v$. In fact the question is a kind of dual: what would we \emph{not} be able to compute if we \emph{lacked} the resources in $v$?

As well as intuitively computing the wrong information, we can also understand the problem in terms of compositionality if we recall that Galois connections themselves are morphisms. Suppose $\Ann{V}_1$ and $\Ann{V}_2$ are the lattices of selections for two views computed from shared data, and $\Ann{D}$ is the lattice of selections for the common data. Using the procedure given in \secref{data-dependencies}, we can obtain two Galois dependency analyses $f: \Ann{V}_1 \to \Ann{D}$ and $g: \Ann{V}_2 \to \Ann{D}$ as shown in \figref{toolkit:de-morgan:non-composable} below:

\input{fig/example/de-morgan.tex}

\noindent Unfortunately, $f$ and $g$ are not composable, as their types makes clear. While the upper adjoint $\upperAdj{g}: \Ann{D} \to \Ann{V}_2$ has the right type to compose with the lower adjoint $\lowerAdj{f}: \Ann{V}_1 \to \Ann{D}$, the result is not a Galois connection: $\upperAdj{g}$ preserves meets, whereas $\lowerAdj{f}$ preserves joins.

\subsection{De Morgan duality}
\label{sec:toolkit:de-morgan-duality}

We now show how the idea of the \emph{complement} of a selection can be used to invert $g$ to yield a Galois connection $\dual{g}$ that it is composable with $f$, and moreover that computes a dependency relation corresponding to necessity rather than sufficiency. By composing $f$ with $\dual{g}$ we obtain a Galois connection linking $\Ann{V}_1$ to $\Ann{V}_2$ via $\Ann{D}$ (\secref{toolkit:de-morgan-duality}). We will also use this understanding to contrast the approach presented in this paper with prior work on program slicing based on Galois connections (\secref{toolkit:galois-slicing}).

First we extend the setting in \secref{data-dependencies} from lattices to Boolean lattices (or Boolean algebras) $\Ann{A}= \BoolLattice{\Ann{A}}{\top}{\bot}{\meet}{\join}{\neg}$, which are lattices equipped with an involution $\neg: \Ann{A} \to \Ann{A}$ called \emph{complement}. Boolean algebras satisfy complementation laws $x \meet \neg x = \bot$ and $x \join \neg x = \top$ and De Morgan equalities $\neg x \meet \neg y = \neg(x \join y)$ and $\neg x \join \neg y = \neg(x \meet y)$. If $\Ann{A}$ is a Boolean algebra, then $\Sel{\raw{v}}{A}$ is also a Boolean algebra, with the Boolean operations, and in particular $\neg_{\raw{v}}: \Sel{\raw{v}}{A} \to \Sel{\raw{v}}{A}$, defined pointwise.

\input{fig/example/convolve}

It is an easy consequence of the complementation and De Morgan laws that any meet-preserving operation $\upperAdj{f}: \Ann{A} \to \Ann{B}$ on Boolean algebras has a join-preserving De Morgan dual $\dual{\upperAdj{f}}: \Ann{A} \to \Ann{B}$ given by $\neg_{\Ann{B}} \after \upperAdj{f} \after \neg_{\Ann{A}}$, whose upper adjoint is $\dual{\lowerAdj{f}}$, the De Morgan dual of $\lowerAdj{f}$. Thus Galois connections on Boolean algebras are closed under De Morgan duality defined component-wise.

\begin{definition}[De Morgan dual of a Galois connection]
   Suppose $\Ann{A}$ and $\Ann{B}$ are Boolean algebras and $f: \Ann{A} \to \Ann{B}$ is a Galois connection $(\lowerAdj{f},\upperAdj{f})$. Define the \emph{De Morgan dual} $\dual{f}$ of $f$ to be the Galois connection $(\dual{\upperAdj{f}}, \dual{\lowerAdj{f}}): \Ann{B} \to \Ann{A}$.
\end{definition}

\noindent De Morgan duality is contravariant, because it swaps the roles of the upper and lower adjoints. So while $f: \Ann{A} \to \Ann{B}$ and $g: \Ann{C} \to \Ann{B}$ are not composable, $f$ and $\dual{g}: \Ann{B} \to \Ann{C}$ are, as illustrated in \figref{toolkit:de-morgan:composable} above.

\subsubsection{Example: matrix convolution}

\figref{example:convolve-viz} shows $\dual{\evalGC{T}}$, contrasting it with $\evalGC{T}$ and showing how it can determine the parts of an output that depend on (are necessary for) an input selection. The example computes the convolution of a matrix with a kernel; the computation has an intuitive dependency structure and an easy visual presentation which makes it useful for conveying the flavour of the two Galois connections. The source code for the example is given in \figref{example:convolve}; our PureScript implementation was used to generate the figures.

\figref{example:convolve-viz:galois-dependency} illustrates the baseline $\evalGC{T}$ Galois connection defined in \secref{data-dependencies}, showing just one of the two possible round-trips. First, the user selects the output cell at position $(2,2)$ (counting rows downwards from $1$). This induces a demand (via the lower adjoint $\evalBwdF{T}$) on the inputs matrix \kw{image} and the kernel \kw{filter}, revealing that the entire kernel was needed to compute the value $1$, but that the elements at $(1,3)$ and $(3,1)$ in the input matrix were not needed, because of zeros present in the kernel. If we then use that input selection to  compute an availability on the output using the upper adjoint $\evalFwdF{T}$, we see that the selection grows: it turns out that the resources needed to make the contents of $(2,2)$ are sufficient to make the contents of $(1,1)$ as well. These larger selections that arise from round-tripping are \emph{closed elements} (fixed points).

\figref{example:convolve-viz:de-morgan-dual} illustrates the De Morgan dual $\dual{\evalGC{T}}$, again showing the round-trip in one direction. Here the user selects kernel cell $(1, 2)$ to see the output cells that depend on it. This is computed by negating the input selection, marking the contents of $(1, 2)$ as unavailable. Forward-analysing with $\evalFwdF{T}$ reveals that if all inputs are available except kernel cell $(1, 2)$, we can compute only the top row of the output. (We can compute the top row even with the missing kernel cell because we used the method \kw{zero} for dealing with boundaries; \kw{wrap} would have given a different behaviour.) Negating that top row selection produces the (yellow) output selection shown in the figure, identifying the output cells which depend on kernel cell ($1, 2)$ because they cannot be computed if that input is unavailable.

If we then use this output selection to compute an input selection using the upper adjoint $\upperAdj{\evalGC{T}}$, the result is not a \emph{demand} in the sense computed by the lower adjoint $\evalBwdF{T}$. Instead of determining the inputs needed for the output selection, it determines the inputs that would \emph{not} be needed if those outputs were not needed: in other words, those inputs that are \emph{only} needed for the output selection. Here the input selection grows, revealing that if kernel cell $(1, 2)$ is unavailable, then the entire top row of the kernel might as well have been unavailable too, and similarly for the bottom 3 rows of the input. Again these larger selections that result from round-tripping are fixed points.

\input{fig/example/convolve-viz}

\subsection{Relationship to Galois slicing}
\label{sec:toolkit:galois-slicing}

With the De Morgan dual, our approach addresses the two main problems we set out to solve in \secref{introduction}. In the next section we show how we can track data dependencies back to related surface language constructors, but first we briefly examine the relationship between the present system and earlier work on ``Galois slicing'', a program slicing technique that has been explored for pure functional programs~\cite{perera12a}, functional programs with effects~\cite{ricciotti17}, and \piCalculus~\cite{perera16d}. We consider other related work in \secref{conclusion}.

Galois slicing has the flavour of what we needed to address these problems, but is unable to compute the kind of dependency relation we gave in \secref{data-dependencies}. Galois slicing works with a notion of \emph{program slice} defined as a program where some subexpressions have been replaced by hole $\hole$. (If we think of a selection, in our sense, as picking out a set of paths in a term, then slices resembles selections which are prefix-closed, meaning that if a given path in a term is selected, then so are all of its prefixes.) Then a meet-preserving \emph{forward-slicing} function (for a fixed computation) is defined which takes a slice of the original program to a slice of its output, a value where some subvalues have been replaced by $\hole$, and dual to this, a join-preserving \emph{backward-slicing} function taking value slices program slice. Forward and backward slicing, for a given computation, form a Galois connection, establishing the important correctness properties that validate the approach.

One problem with this approach is that it does not readily extend to a notion of selection where the part of the output of interest is not a prefix of the output, but rather a prefix of some subtree. \emph{Differential} slicing \cite{perera12a} uses Galois slicing to compute a pair of $(e,e'$) for a pair of output slices $(v,v')$ where $v \leq v'$. By monotonicity, $e \leq e'$. This can be used to compute a slice for a subtree, by setting $v'$ to be the ``spine'' of the original output up to the location of the subtree with a $\hole$ instead of the subtree, and $v'$ is $v$ with the subtree plugged into that position. For example, consider the program shown in \figref{example:diff-slicing}, which has the value \lstinline{(0.4, 0.6)}. Because differential slicing includes a program part if is needed \emph{only} by the selected output, in general it underapproximates the parts actually needed for the selected output. In this example, \lstinline{2} and \lstinline{3} are both needed to compute the spine containing the subtree of interest, and so the differential slice does not include them. Differential slicing based on tree prefixes is thus not a suitable technique for computing data dependencies.

\input{fig/example/diff-slicing}

Our approach does have one disadvantage vis-\'a-vis Galois slicing, namely that a program ``selection'' does not really resemble a program, but merely picks out various constants and constructors in the program involved in constructing the output selection. With Galois slicing, the program slice is a program with some holes. Although it is not executable as-is, one could (for example) imagine replacing holes by arbitrary expressions of an appropriate type, in order to recover an executable slice. It is not clear with our approach how to ``extract'' an executable slice for a particular output selection; for primitive values, one could extract the \emph{expression provenance} \cite{acar12}, which would explain how the primitive value was computed using primitive operations, but it is not easy to see how this generalises to structured outputs. Moreover there is no property that ensures the expression provenance is in some sense a projection of the semantics of the original program; \cite{field98} explore this notion of executable slice in the context of term rewriting systems, so perhaps this idea could be adapted to our core calculus and used to derive a notion of execution slice. \todo{needs work -- perhaps move this paragraph to Related work}
